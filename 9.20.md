# 11.15

In [354]: la = ['10','1','2','3']                                                           
In [355]: sorted(la)                                                                                      
Out[355]: ['1', '10', '2', '3']


ModuleNotFoundError: No module named 'pyinotify'  
mac å®‰è£…ä¸äº†ï¼Œï¼Œubuntu å¯ä»¥

ps -ef | grep 

crontab  å®šæ—¶ä»»åŠ¡ï¼Œ  æ—¥å¿—å‘¢
æ€ä¹ˆå…³é—­
sudo chmod +x my.sh

sudo vim /etc/crontab

crontab /etc/crontab  å¯åŠ¨
crontab â€“l : æ˜¾ç¤º crontab æ–‡ä»¶
Macç³»ç»Ÿä¸‹

sudo /usr/sbin/cron start
sudo /usr/sbin/cron restart
sudo /usr/sbin/cron stop
Ubuntu:

sudo /etc/init.d/cron start
sudo /etc/init.d/cron stop
sudo /etc/init.d/cron restart

é¦–å…ˆ vim  crontest ä¸€ä¸ªæ–‡ä»¶
æ¯”å¦‚å†…å®¹æ˜¯è¿™æ ·çš„
  1 #
  2 30 8 * * * nohup python -u /data/turing/timer_del_model.py >> /data/turing/timer_del_model.log 2>&1 &

æ¯å¤©8ç‚¹åŠ  å®šæ—¶æ‰§è¡Œè¿™ä¸ªä»»åŠ¡ï¼Œ

ç„¶å crontab crontest  å¯åŠ¨ä»»åŠ¡äº†
çœ‹ä»»åŠ¡åˆ—è¡¨  crontab -l
è‹¥ä¿®æ”¹ï¼Œåˆ™ç›´æ¥æ”¹æ–‡ä»¶å³å¯



#
30 10 * * * nohup python3 -u /data/zhangkl/turing_new/handle.bgru_online.py >> /data/zhangkl/turing_new/online.log 2>&1 &


  redis æœåŠ¡å¼€å¯ï¼Œpythonredis clintæ–¹å¯ç”¨
å®‰è£…çš„æ—¶å€™
pythonå®¢æˆ·ç«¯ pip install redis
æœåŠ¡ç«¯
wget http://download.redis.io/redis-stable.tar.gz
tar xvzf redis-stable.tar.gz
cd redis-stable
make
make test åˆ«å¿˜äº†

pwd
/Users/zhangkailin/Downloads/redis-stable

redis å¼€å¯å‘½ä»¤ï¼šsrc/redis-server


20360:C 12 Nov 2019 14:39:58.098 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
20360:C 12 Nov 2019 14:39:58.098 # Redis version=5.0.6, bits=64, commit=00000000, modified=0, pid=20360, just started
20360:C 12 Nov 2019 14:39:58.098 # Warning: no config file specified, using the default config. In order to specify a config file use src/redis-server /path/to/redis.conf
20360:M 12 Nov 2019 14:39:58.099 * Increased maximum number of open files to 10032 (it was originally set to 256).


pandas merge  left è¡¨ç¤º å·¦è¾¹çš„æ‰€æœ‰ ä¿ç•™ï¼Œ    
æ¯”å¦‚ a.merge(b,left on ==4)
a (400,10), b(1000,5)
a(<400,11)  å¤šåŠ çš„ä¸€ä¸ªï¼Œæœ‰çš„ä¸å˜ï¼Œæ²¡æœ‰çš„nan
pandas groupby 



#ä¸¤ä¸ªåº“
AlitaNet:
 A Click Through Rate (ctr) prediction Network implementation with TensorFlow, including LR, FM, NFM, AFM, Wide&Deep, DeepFM, xDeepFM, AutoInt, FiBiNet, LS-PLM, DCN, etc.

Surprise    A Python scikit for building and analyzing recommender systems

xdlä¸ºå•¥ä¸ç”¨

xLearn is a high performance, easy-to-use, and scalable machine learning package that contains linear model (LR), factorization machines (FM), and field-aware factorization machines (FFM), all of which can be used to solve large-scale machine learning problems.

tt.columns.tolist()

1.2 åˆ†æç›®çš„

åˆ†æå¾—å‡ºä¸»è¦ç”µå½±ç±»å‹ï¼ˆå¦‚ç”µå½±æ•°é‡300ä¸‡ä»¥ä¸Šçš„ç”µå½±ç±»å‹ï¼‰
åˆ†æä¸»è¦ç”µå½±ç±»å‹è¯„åˆ†çš„å˜åŒ–è¶‹åŠ¿
åˆ†æä¸»è¦ç”µå½±ç±»å‹è¯„åˆ†å˜åŒ–è¶‹åŠ¿ä¹‹é—´çš„è”ç³»
åˆ†æç”¨æˆ·å¯¹ä¸åŒç±»å‹ç”µå½±è¯„åˆ†ä¹‹é—´çš„å…³ç³»å¼ºåº¦
1.3 åˆ†æé—®é¢˜

ä¸åŒç±»å‹ç”µå½±çš„å¹³å‡è¯„åˆ†å˜åŒ–è¶‹åŠ¿ï¼š
    ä¸»è¦ç”µå½±ç±»å‹çš„å¹³å‡è¯„åˆ†å¦‚ä½•å˜åŒ–ï¼Ÿ
    å¯¹ä¸åŒç±»å‹ç”µå½±çš„å¹³å‡è¯„åˆ†è¿›è¡Œæ¯”è¾ƒä¼šæœ‰ä»€ä¹ˆå‘ç°ï¼Ÿ
ä¸åŒç±»å‹ç”µå½±å¹³å‡è¯„åˆ†å˜åŒ–è¶‹åŠ¿ä¹‹é—´çš„å…³è”ç¨‹åº¦ï¼š
    ä¸åŒç±»å‹ç”µå½±å¹³å‡è¯„åˆ†çš„å˜åŒ–è¶‹åŠ¿ä¹‹é—´æœ‰å…³è”å—ï¼Ÿ
    æ¯”å¦‚ï¼Œå–œå‰§ç‰‡ï¼ˆComedyï¼‰å’Œå†’é™©ç‰‡ï¼ˆAdventureï¼‰ç­‰å…¶ä»–ç”µå½±ç±»å‹çš„å¾—åˆ†ä¹‹é—´æ˜¯æ­£ç›¸å…³ï¼Ÿå…³è”å¼ºåº¦æ€ä¹ˆæ ·ï¼Ÿ




çŸ©é˜µåˆ†è§£ï¼ˆMatrix factorizationï¼‰

  åŸºäºéšå˜é‡çš„æ¨èï¼ˆLatent Factor Modelï¼‰

 æ–¹é˜µ ç‰¹å¾åˆ†è§£ æŒ‰ç‰¹å¾å€¼ç‰¹å¾å‘é‡åˆ†è§£ï¼Œ 
 ä¸æ˜¯æ–¹é˜µï¼Œsvdå¥‡å¼‚å€¼çŸ©é˜µåˆ†è§£
 SVDå¯ä»¥ç”¨äºPCAé™ç»´ï¼Œæ¥åšæ•°æ®å‹ç¼©å’Œå»å™ªã€‚ä¹Ÿå¯ä»¥ç”¨äºæ¨èç®—æ³•ï¼Œå°†ç”¨æˆ·å’Œå–œå¥½å¯¹åº”çš„çŸ©é˜µåšç‰¹å¾åˆ†è§£ï¼Œè¿›è€Œå¾—åˆ°éšå«çš„ç”¨æˆ·éœ€æ±‚æ¥åšæ¨èã€‚åŒæ—¶ä¹Ÿå¯ä»¥ç”¨äºNLPä¸­çš„ç®—æ³•ï¼Œæ¯”å¦‚æ½œåœ¨è¯­ä¹‰ç´¢å¼•ï¼ˆLSIï¼‰

çŸ©é˜µåˆ†è§£ç®—æ³•ç”±å¥‡å¼‚å€¼åˆ†è§£ç®—æ³•ï¼ˆSingular Value Decomposition, SVDï¼‰æ¼”å˜è€Œæ¥ï¼Œä¼ ç»Ÿçš„å¥‡å¼‚å€¼åˆ†è§£ç®—æ³•åªèƒ½å¯¹æ•°æ®ç¨ å¯†çš„çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼Œè€Œè¯„åˆ†çŸ©é˜µæ˜¯æåº¦ç¨€ç–çš„ï¼Œå› æ­¤ï¼Œè‹¥è¦ä½¿ç”¨SVDå¯¹è¯„åˆ†çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼Œé¦–å…ˆè¦å¯¹çŸ©é˜µçš„ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ï¼Œè¿™æ ·ä¾¿é€ æˆäº†ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼š

Â  å¡«å……ç¼ºå¤±æ•°æ®ä¼šæå¤§çš„å¢åŠ æ•°æ®é‡ï¼Œå¯¼è‡´ç®—æ³•å¤æ‚åº¦ä¸Šå‡ã€‚
Â  å¡«å……æ–¹æ³•ä¸å½“ä¼šå¯¼è‡´æ•°æ®å¤±çœŸã€‚
ç”±äºSVDç®—æ³•åœ¨è¯„åˆ†çŸ©é˜µä¸­ä¸èƒ½å‘æŒ¥è‰¯å¥½çš„ä½œç”¨ï¼Œäººä»¬è½¬è€Œç ”ç©¶æ˜¯å¦èƒ½åªè€ƒè™‘å·²æœ‰è¯„åˆ†å¯¹çŸ©é˜µè¿›è¡Œåˆ†è§£ï¼Œäºæ˜¯ä¾¿æœ‰äº†BasicSVDã€FunkSVDã€SVD++ç­‰çŸ©é˜µåˆ†è§£æ–¹æ³•ã€‚

ï¼ˆä¸€ï¼‰BasicSVD

ï¼ˆäºŒï¼‰FunkSVD

ï¼ˆä¸‰ï¼‰Baseline estimates & Matrix factorization

ï¼ˆå››ï¼‰Asymmetric-SVD

ï¼ˆäº”ï¼‰SVD++

åœ¨ä¼ ç»Ÿçš„çº¿æ€§æ¨¡å‹å¦‚LRä¸­ï¼Œæ¯ä¸ªç‰¹å¾éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå¦‚æœéœ€è¦è€ƒè™‘ç‰¹å¾ä¸ç‰¹å¾ç›´æ¥çš„äº¤äº’ä½œç”¨ï¼Œå¯èƒ½éœ€è¦äººå·¥å¯¹ç‰¹å¾è¿›è¡Œäº¤å‰ç»„åˆï¼›éçº¿æ€§SVMå¯ä»¥å¯¹ç‰¹å¾è¿›è¡Œkernelæ˜ å°„ï¼Œä½†æ˜¯åœ¨ç‰¹å¾é«˜åº¦ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸èƒ½å¾ˆå¥½åœ°è¿›è¡Œå­¦ä¹ ï¼›ç°åœ¨ä¹Ÿæœ‰å¾ˆå¤šåˆ†è§£æ¨¡å‹Factorization modelå¦‚çŸ©é˜µåˆ†è§£MFã€SVD++ç­‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ç‰¹å¾ä¹‹é—´çš„äº¤äº’éšè—å…³ç³»ï¼Œä½†åŸºæœ¬ä¸Šæ¯ä¸ªæ¨¡å‹éƒ½åªé€‚ç”¨äºç‰¹å®šçš„è¾“å…¥å’Œåœºæ™¯ã€‚ä¸ºæ­¤ï¼Œåœ¨é«˜åº¦ç¨€ç–çš„æ•°æ®åœºæ™¯ä¸‹å¦‚æ¨èç³»ç»Ÿï¼ŒFMï¼ˆFactorization Machineï¼‰å‡ºç°äº†

å…¶ä»–åˆ†è§£æ¨¡å‹åŒ…æ‹¬Matrix factorization (MF)ã€SVD++ã€PITF for Tag Recommendationã€Factorized Personalized Markov Chains (FPMC)ï¼Œè¿™äº›æ¨¡å‹éƒ½åªåœ¨ç‰¹å®šåœºæ™¯ä¸‹ä½¿ç”¨ï¼Œè¾“å…¥å½¢å¼ä¹Ÿæ¯”è¾ƒå•ä¸€ï¼ˆæ¯”å¦‚MFåªé€‚ç”¨äºcategorical variablesï¼‰ï¼Œè€ŒFMé€šè¿‡å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œè½¬æ¢ï¼ŒåŒæ ·å¯å¯ä»¥å®ç°ä»¥ä¸Šæ¨¡å‹çš„åŠŸèƒ½ï¼Œè€Œä¸”FMçš„è¾“å…¥å¯ä»¥æ˜¯ä»»æ„å®æ•°åŸŸçš„æ•°æ®ï¼Œå› æ­¤FMæ˜¯ä¸€ä¸ªæ›´ä¸ºæ³›åŒ–å’Œé€šç”¨çš„æ¨¡å‹


æœ‰è¯„åˆ†çŸ©é˜µï¼Œï¼Œå­¦ä¹ svdåˆ†è§£ ä¸¤ä¸ªçŸ©é˜µï¼Œç‚¹ä¹˜   å•†å“å±æ€§-å•†å“ çŸ©é˜µï¼Œå•†å“å±æ€§-ç”¨æˆ· çŸ©é˜µ  è¯¯å·®å‡½æ•°ï¼Œ éšæœºæ¢¯åº¦ä¸‹é™
è¡Œä¸ºç”¨æˆ·ï¼Œåˆ—ä¸ºå•†å“


ååŒè¿‡æ»¤ç®—æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼Œä¸€ç±»æ˜¯åŸºäºé¢†åŸŸçš„æ–¹æ³•(neighborhood methods)ï¼Œå¦ä¸€ç±»æ˜¯éšè¯­ä¹‰æ¨¡å‹(latent factor models)ï¼Œåè€…ä¸€ä¸ªæœ€æˆåŠŸçš„å®ç°å°±æ˜¯çŸ©é˜µåˆ†è§£(matrix factorization)ï¼ŒçŸ©é˜µåˆ†è§£æˆ‘ä»¬è¿™ç¯‡æ–‡ç« ä½¿ç”¨çš„æ–¹æ³•å°±æ˜¯SVD(å¥‡å¼‚å€¼åˆ†è§£)

æé—®â“ï¼šSVDåœ¨æ¨èç³»ç»Ÿä¸­åˆ°åº•åœ¨ä»€ä¹ˆä½ç½®å‘¢ï¼Ÿ

ä¸¾æ‰‹ğŸ™‹â€â™‚ï¸ï¼šæ¨èç³»ç»Ÿ -> ååŒè¿‡æ»¤ç®—æ³• -> éšè¯­ä¹‰æ¨¡å‹ -> çŸ©é˜µåˆ†è§£ -> SVD


ctrè€Œæ˜¯ä»lr å¼€å§‹ fm  ffm
fm  æ˜¯ ç›´æ¥ä» åŸå§‹ç‰¹å¾å…¥æ‰‹ï¼Œ ä¸¤ä¸ªçŸ©é˜µï¼Œ æ„é€ å‡º è¯„åˆ†çŸ©é˜µï¼Œ


#ç‰¹å¾ 
lr  fm ffm   éƒ½æ˜¯ç›´æ¥ç‰¹å¾  ä¹˜ä»¥ w  æ²¡æœ‰onehotçš„  

gbdt+lr å‘¢    gbdt ä¸­é—´å‘¢

deepFM onehotå¼€å¼€å§‹æ˜¯å—

FTRLä¸€è·¯èµ°æ¥ï¼Œä»LR -> SGD -> TG -> FOBOS -> RDA -> FTRL


Yahoo [8, 11, 29], Etsy [1], Criteo [18], Linkedin [15, 23], Tinder [16], Tumblr [10], Instacart [22], Facebook [28].
3



#init_method: normal,tnormal,uniform,he_normal,he_uniform,xavier_normal,xavier_uniform
#metric :'auc','logloss', 'group_auc'


 ä¸è¦åœ¨ç”¨feed_dict å–‚å…¥æ•°æ®äº†ï¼Œæ…¢
ç”¨tf.data å†…åµŒçš„

use an input pipeline to ensure that the GPU has never to wait for new stuff to come in
step1: Importing Data. Create a Dataset instance from some data
step2: Create an Iterator. By using the created dataset to make an Iterator instance to iterate through the dataset
step3: Consuming Data. By using the created iterator we can get the elements from the dataset to feed the model

x = np.random.sample((5,2))
tf.data.Dataset.from_tensor_slices(x)
å¯¹ä¼ å…¥çš„ï¼ˆ5,2ï¼‰è¿›è¡Œåˆ‡åˆ†ï¼Œæœ€ç»ˆäº§ç”Ÿçš„datasetæœ‰5ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ çš„å½¢çŠ¶éƒ½æ˜¯(2,)


tf.data.Dataset.from_tensor_slices( )


filenames = tf.placeholder(tf.string, shape=[None])
#src_dataset = tf.contrib.data.TFRecordDataset(filenames)
src_dataset = tf.data.TFRecordDataset(filenames)
_parse_functionæ˜¯è§£æTFRecordsçš„å‡½æ•°

make_initializable_iterator

f.decode_rawä¸tf.castçš„åŒºåˆ«
tf.decode_rawå‡½æ•°çš„æ„æ€æ˜¯å°†åŸæ¥ç¼–ç ä¸ºå­—ç¬¦ä¸²ç±»å‹çš„å˜é‡é‡æ–°å˜å›æ¥ï¼Œè¿™ä¸ªæ–¹æ³•åœ¨æ•°æ®é›†datasetä¸­å¾ˆå¸¸ç”¨ï¼Œå› ä¸ºåˆ¶ä½œå›¾ç‰‡æºæ•°æ®ä¸€èˆ¬å†™è¿›tfrecordé‡Œç”¨to_bytesçš„å½¢å¼ï¼Œä¹Ÿå°±æ˜¯å­—ç¬¦ä¸²ã€‚è¿™é‡Œå°†åŸå§‹æ•°æ®å–å‡ºæ¥ å¿…é¡»åˆ¶å®šåŸå§‹æ•°æ®çš„æ ¼å¼ï¼ŒåŸå§‹æ•°æ®æ˜¯ä»€ä¹ˆæ ¼å¼è¿™é‡Œè§£æå¿…é¡»æ˜¯ä»€ä¹ˆæ ¼å¼ï¼Œè¦ä¸ç„¶ä¼šå‡ºç°å½¢çŠ¶çš„ä¸å¯¹åº”é—®é¢˜


TFRecordDatasetå’Œtf.data.Datasetéå¸¸ç›¸ä¼¼


tf.FixedLenFeature(




æ•°æ®å¹¶è¡Œç§°ä¹‹ä¸º"between-graph replication"
æ¨¡å‹å¹¶è¡Œç§°ä¹‹ä¸º"in-graph replication"

æ•°æ®å¹¶è¡Œå¯ä»¥æ˜¯åŒæ­¥çš„ï¼ˆsynchronousï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å¼‚æ­¥çš„ï¼ˆasynchronousï¼‰




airbnb  ä¸ªæ€§åŒ–å®æ—¶æ¨è æˆ¿æºç›¸ä¼¼é¡µï¼Œï¼Œæœç´¢é¡µï¼Œï¼Œword2vec
item2vec æ¨è
deepwalk node2vec å›¾åµŒå…¥å‘é‡ ç¤¾äº¤ç½‘ç»œ


æ¨èçš„å››å¤§é¡¶ä¼š
å››å¤§é¡¶ä¼šï¼ˆKDDã€SIGIRã€TheWebConf WWWå’ŒRecSysï¼‰
A* in the Australian Core Ranking


NLP é¡¶ä¼š
NAACLï¼ŒACL

cvé¡¶ä¼š
cvpr

é€šç”¨æœºå™¨å­¦ä¹ é¡¶ä¼š
ICLRã€ICMLï¼ŒECCV,NIPS,  ICCV , EMNLP   NLPCC

â€˜layersâ€™ -> â€˜slimâ€™ -> â€˜estimatorsâ€™ -> â€˜tf.kerasâ€™

ACM International Conference on Web Search and Data Mining (WSDMâ€™


Airbnb çš„ Real-time Personalization using Embeddings for Search Ranking at Airbnb ä¸€æ–‡æ‹¿äº†ä»Šå¹´ KDD ADS Track çš„æœ€ä½³è®ºæ–‡ï¼Œå’Œ 16 å¹´ Google çš„ W&D ç±»ä¼¼ï¼Œå¹¶ä¸ fancyï¼Œä½†éå¸¸ practicable


deepfm
            # model
            # -1 256 8    -1 39  =  -1 39*8  
            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K
            # -1  39   =ã€‹  -1  39 1
            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])
            #-1*39*8   -1*39*1å¹¿æ’­ï¼Œ = -1 *39*8
            self.embeddings = tf.multiply(self.embeddings,feat_value)


me

                #20*40*128
                self.ad_img_embeddings_var  = tf.get_variable("ad_img_embedding_var", [AD_IMG_LABEL_DIM,AD_IMG_VALUE_DIM,EMBEDDING_DIM])
                #ç´¢å¼• idx     -1 *4*ï¼ˆ40*128ï¼‰
                self.ad_img_embedded = tf.nn.embedding_lookup(self.ad_img_embeddings_var, self.ad_label_ph)
                #  -1*4    -1*4*40
                self.ad_value_ph_ohot = tf.one_hot(self.ad_value_ph,depth=AD_IMG_VALUE_DIM,axis=-1)
                # -1*4*40   -1*4*1*40
                self.ad_value_ph_ohot = tf.expand_dims(self.ad_value_ph_ohot,axis=-2)
                #n*7*8*128  å°±æ˜¯å¯¹åº”ç›¸ä¹˜ï¼Œ  -1*4*1*40  -1 *4*ï¼ˆ40*128ï¼‰= -1 *4*1*128
                self.ad_img_embedded = tf.matmul(self.ad_value_ph_ohot ,self.ad_img_embedded)        
                self.ad_img_eb = self.ad_img_embedded     # none*n*1*128     
                self.ad_img_eb = tf.squeeze(self.ad_img_eb,[-2])  #n*n*128

                self.ad_img_eb_sum = tf.reduce_sum(self.ad_img_eb,-2)



multi lable
            for s in Config.multi_features:
                temp_multi_result = tf.nn.embedding_lookup_sparse(self.vr['multi_first_embedding_%s' % s],
                tf.SparseTensor(indices=self.ph['multi_index_%s' % s],
                                values=self.ph['multi_value_%s' % s],
                                dense_shape=(Config.batch_size,
                                            Config.embedding_size)),
                None,
                combiner="sum"
                )

### embedding lookup SparseTensor
idx = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 1], [1, 2], [2, 0]],
                      values=[0, 1, 2, 3, 0], dense_shape=[3, 3])
# è¿™ä¸ªç¨€ç–çŸ©é˜µå†™æˆæ™®é€šå½¢å¼è¿™æ ·
#---------------------------------------------------------------------#
array([[0, 1, None],     æ¯ä¸€è¡Œçš„æ¯åˆ— æ‹¿å‡ºåµŒå…¥å‘é‡ï¼Œç›¸åŠ 
       [None, 2, 3],
       [0, None, None]]) # ä¸ºäº†ä¸0å…ƒç´ ç›¸åŒºåˆ«ï¼Œæ²¡æœ‰å¡«å……çš„éƒ¨åˆ†å†™æˆäº†None 



import pymysql

conn = pymysql.connect(host = "218.68.6.114",user = "leapmotion",passwd = 'leapmotion',db = 'leapmotion_data',port =3306)
cur = conn.cursor()

cur.execute('select * from LeapMachine where id <=100')

'''
'(('{json}',id))'
'''
data_all = cur.fetchall()



    # cell = tf.nn.rnn_cell.LSTMCell(HIDDEN_DIM)
    # cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=KEEP_PROB)
    # cells = [cell for _ in range(NUM_LAYERS)]

    def build_cell(n,m):
        cell = tf.nn.rnn_cell.LSTMCell(n)
        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=m)
        return cell

    num_units=[HIDDEN_DIM*2,HIDDEN_DIM]
    
    cells = [build_cell(n,KEEP_PROB) for n in num_units]
    Cell_stacked = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)


å­—å…¸
    for k, v in aa.items():

    pandas 
    for row in aim.itertuples(index=False):
                        if getattr(row, "label") == 1:



In [102]: data                                                  
Out[102]: {'a': [1, 2, 3], 'c': [4, 5, 6], 'b': [7, 8, 9]}

In [103]: frame = pd.DataFrame(data,index=['one','two','three'])                                                                                             
In [104]: frame                                                                                                
Out[104]: 
       a  c  b
one    1  4  7
two    2  5  8
three  3  6  9

In [110]: for i in range(len(frame)): 
     ...:     tmp = frame.iloc[i]['b'] 
     ...:     print(tmp) 
     ...:                          
7
8
9
è¿™ä¸ªä¼šæŠ¥é”™
# for i in range(len(frame["one"])):
#     print(a["one"][i])



In [162]: tag =2                                                                        
In [163]: for i in range(10): 
     ...:     print(i) 
     ...:     if i ==tag: 
     ...:         break 
     ...: else: 
     ...:     print("xx") 
     ...: print("ok")                                    
0
1
2
ok

In [164]: for i in range(1): 
     ...:     print(i) 
     ...:     if i ==tag: 
     ...:         break 
     ...: else: 
     ...:     print("xx") 
     ...: print("ok")                                                                                             
0
xx
ok


#dien
with tf.name_scope('rnn_1'):
    # self.seq_len_ph ä¸€ç»´ï¼Œæ˜¯æ¯ä¸ªå†å²æ•°æ®ï¼Œçš„é•¿åº¦ï¼Œå®é™…é•¿åº¦ï¼Œä¸æ˜¯æœ€å¤§é•¿åº¦ï¼Œï¼Œé‡Œé¢ ç”¨ä½œmaskçš„
    rnn_outputs, _ = dynamic_rnn(GRUCell(HIDDEN_SIZE), inputs=self.item_his_eb,
                                    sequence_length=self.seq_len_ph, dtype=tf.float32,
                                    scope="gru1")
with tf.name_scope('Attention_layer_1'):
    #self.mask_ph äºŒç»´çŸ©é˜µï¼Œæ ·æœ¬ æœ€å¤§é•¿åº¦ï¼Œæœ‰å€¼å¾—ä¸º1 å…¶ä»– ä¸ºé›¶   
    #æ³¨æ„  è¿™é‡Œçš„maskï¼Œæ˜¯å°¾éƒ¨ä¸ºé›¶ï¼Œ   è€Œå†å²ä¿¡æ¯
    att_outputs, alphas = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask_ph,
                                            softmax_stag=1, stag='1_1', mode='LIST', return_alphas=True)
with tf.name_scope('rnn_2'):
    rnn_outputs2, final_state2 = dynamic_rnn(VecAttGRUCell(HIDDEN_SIZE), inputs=rnn_outputs,
                                                att_scores=tf.expand_dims(alphas, -1),
                                                sequence_length=self.seq_len_ph, dtype=tf.float32,
                                                scope="gru2")



*/5 * * * * nohup python -u /data/go/src/naga/songofsiren/tools/log/report_api_error.py >> /data/logs/naga/report_by_log.log  2>&1 &
#*/5 * * * * nohup python -u /data/go/src/naga/songofsiren/tools/send_by_online.py >> /data/go/src/naga/songofsiren/tools/send_by_online.log  2>&1 &
0 */1 * * * nohup python -u /data/go/src/naga/songofsiren/tools/report/report_by_susuan_ocrtask.py >> /data/logs/naga/report_by_susaun_ocrtask.log  2>&1 &
0 2 * * * bash /disk/script/online_log_bak.sh >> /disk/script/log/online_log_bak.log 2>&1 &
0 4 * * * nohup python3 /disk/script/parser_log_insert_to_ocrdb.py /disk/disk2 20190101-20191232  >> /disk/script/log/parser_log_insert_to_ocrdb.log 2>&1 &