# 11.15

In [354]: la = ['10','1','2','3']                                                           
In [355]: sorted(la)                                                                                      
Out[355]: ['1', '10', '2', '3']


ModuleNotFoundError: No module named 'pyinotify'  
mac 安装不了，，ubuntu 可以

ps -ef | grep 

crontab  定时任务，  日志呢
怎么关闭
sudo chmod +x my.sh

sudo vim /etc/crontab

crontab /etc/crontab  启动
crontab –l : 显示 crontab 文件
Mac系统下

sudo /usr/sbin/cron start
sudo /usr/sbin/cron restart
sudo /usr/sbin/cron stop
Ubuntu:

sudo /etc/init.d/cron start
sudo /etc/init.d/cron stop
sudo /etc/init.d/cron restart

首先 vim  crontest 一个文件
比如内容是这样的
  1 #
  2 30 8 * * * nohup python -u /data/turing/timer_del_model.py >> /data/turing/timer_del_model.log 2>&1 &

每天8点半  定时执行这个任务，

然后 crontab crontest  启动任务了
看任务列表  crontab -l
若修改，则直接改文件即可



#
30 10 * * * nohup python3 -u /data/zhangkl/turing_new/handle.bgru_online.py >> /data/zhangkl/turing_new/online.log 2>&1 &


  redis 服务开启，pythonredis clint方可用
安装的时候
python客户端 pip install redis
服务端
wget http://download.redis.io/redis-stable.tar.gz
tar xvzf redis-stable.tar.gz
cd redis-stable
make
make test 别忘了

pwd
/Users/zhangkailin/Downloads/redis-stable

redis 开启命令：src/redis-server


20360:C 12 Nov 2019 14:39:58.098 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
20360:C 12 Nov 2019 14:39:58.098 # Redis version=5.0.6, bits=64, commit=00000000, modified=0, pid=20360, just started
20360:C 12 Nov 2019 14:39:58.098 # Warning: no config file specified, using the default config. In order to specify a config file use src/redis-server /path/to/redis.conf
20360:M 12 Nov 2019 14:39:58.099 * Increased maximum number of open files to 10032 (it was originally set to 256).


pandas merge  left 表示 左边的所有 保留，    
比如 a.merge(b,left on ==4)
a (400,10), b(1000,5)
a(<400,11)  多加的一个，有的不变，没有的nan
pandas groupby 



#两个库
AlitaNet:
 A Click Through Rate (ctr) prediction Network implementation with TensorFlow, including LR, FM, NFM, AFM, Wide&Deep, DeepFM, xDeepFM, AutoInt, FiBiNet, LS-PLM, DCN, etc.

Surprise    A Python scikit for building and analyzing recommender systems

xdl为啥不用

xLearn is a high performance, easy-to-use, and scalable machine learning package that contains linear model (LR), factorization machines (FM), and field-aware factorization machines (FFM), all of which can be used to solve large-scale machine learning problems.

tt.columns.tolist()

1.2 分析目的

分析得出主要电影类型（如电影数量300万以上的电影类型）
分析主要电影类型评分的变化趋势
分析主要电影类型评分变化趋势之间的联系
分析用户对不同类型电影评分之间的关系强度
1.3 分析问题

不同类型电影的平均评分变化趋势：
    主要电影类型的平均评分如何变化？
    对不同类型电影的平均评分进行比较会有什么发现？
不同类型电影平均评分变化趋势之间的关联程度：
    不同类型电影平均评分的变化趋势之间有关联吗？
    比如，喜剧片（Comedy）和冒险片（Adventure）等其他电影类型的得分之间是正相关？关联强度怎么样？




矩阵分解（Matrix factorization）

  基于隐变量的推荐（Latent Factor Model）

 方阵 特征分解 按特征值特征向量分解， 
 不是方阵，svd奇异值矩阵分解
 SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）

矩阵分解算法由奇异值分解算法（Singular Value Decomposition, SVD）演变而来，传统的奇异值分解算法只能对数据稠密的矩阵进行分解，而评分矩阵是极度稀疏的，因此，若要使用SVD对评分矩阵进行分解，首先要对矩阵的缺失值进行填充，这样便造成了以下两个问题：

  填充缺失数据会极大的增加数据量，导致算法复杂度上升。
  填充方法不当会导致数据失真。
由于SVD算法在评分矩阵中不能发挥良好的作用，人们转而研究是否能只考虑已有评分对矩阵进行分解，于是便有了BasicSVD、FunkSVD、SVD++等矩阵分解方法。

（一）BasicSVD

（二）FunkSVD

（三）Baseline estimates & Matrix factorization

（四）Asymmetric-SVD

（五）SVD++

在传统的线性模型如LR中，每个特征都是独立的，如果需要考虑特征与特征直接的交互作用，可能需要人工对特征进行交叉组合；非线性SVM可以对特征进行kernel映射，但是在特征高度稀疏的情况下，并不能很好地进行学习；现在也有很多分解模型Factorization model如矩阵分解MF、SVD++等，这些模型可以学习到特征之间的交互隐藏关系，但基本上每个模型都只适用于特定的输入和场景。为此，在高度稀疏的数据场景下如推荐系统，FM（Factorization Machine）出现了

其他分解模型包括Matrix factorization (MF)、SVD++、PITF for Tag Recommendation、Factorized Personalized Markov Chains (FPMC)，这些模型都只在特定场景下使用，输入形式也比较单一（比如MF只适用于categorical variables），而FM通过对输入特征进行转换，同样可可以实现以上模型的功能，而且FM的输入可以是任意实数域的数据，因此FM是一个更为泛化和通用的模型


有评分矩阵，，学习svd分解 两个矩阵，点乘   商品属性-商品 矩阵，商品属性-用户 矩阵  误差函数， 随机梯度下降
行为用户，列为商品


协同过滤算法主要分为两类，一类是基于领域的方法(neighborhood methods)，另一类是隐语义模型(latent factor models)，后者一个最成功的实现就是矩阵分解(matrix factorization)，矩阵分解我们这篇文章使用的方法就是SVD(奇异值分解)

提问❓：SVD在推荐系统中到底在什么位置呢？

举手🙋‍♂️：推荐系统 -> 协同过滤算法 -> 隐语义模型 -> 矩阵分解 -> SVD


ctr而是从lr 开始 fm  ffm
fm  是 直接从 原始特征入手， 两个矩阵， 构造出 评分矩阵，


#特征 
lr  fm ffm   都是直接特征  乘以 w  没有onehot的  

gbdt+lr 呢    gbdt 中间呢

deepFM onehot开开始是吗

FTRL一路走来，从LR -> SGD -> TG -> FOBOS -> RDA -> FTRL


Yahoo [8, 11, 29], Etsy [1], Criteo [18], Linkedin [15, 23], Tinder [16], Tumblr [10], Instacart [22], Facebook [28].
3



#init_method: normal,tnormal,uniform,he_normal,he_uniform,xavier_normal,xavier_uniform
#metric :'auc','logloss', 'group_auc'


 不要在用feed_dict 喂入数据了，慢
用tf.data 内嵌的

use an input pipeline to ensure that the GPU has never to wait for new stuff to come in
step1: Importing Data. Create a Dataset instance from some data
step2: Create an Iterator. By using the created dataset to make an Iterator instance to iterate through the dataset
step3: Consuming Data. By using the created iterator we can get the elements from the dataset to feed the model

x = np.random.sample((5,2))
tf.data.Dataset.from_tensor_slices(x)
对传入的（5,2）进行切分，最终产生的dataset有5个元素，每个元素的形状都是(2,)


tf.data.Dataset.from_tensor_slices( )


filenames = tf.placeholder(tf.string, shape=[None])
#src_dataset = tf.contrib.data.TFRecordDataset(filenames)
src_dataset = tf.data.TFRecordDataset(filenames)
_parse_function是解析TFRecords的函数

make_initializable_iterator

f.decode_raw与tf.cast的区别
tf.decode_raw函数的意思是将原来编码为字符串类型的变量重新变回来，这个方法在数据集dataset中很常用，因为制作图片源数据一般写进tfrecord里用to_bytes的形式，也就是字符串。这里将原始数据取出来 必须制定原始数据的格式，原始数据是什么格式这里解析必须是什么格式，要不然会出现形状的不对应问题


TFRecordDataset和tf.data.Dataset非常相似


tf.FixedLenFeature(




数据并行称之为"between-graph replication"
模型并行称之为"in-graph replication"

数据并行可以是同步的（synchronous），也可以是异步的（asynchronous）




airbnb  个性化实时推荐 房源相似页，，搜索页，，word2vec
item2vec 推荐
deepwalk node2vec 图嵌入向量 社交网络


推荐的四大顶会
四大顶会（KDD、SIGIR、TheWebConf WWW和RecSys）
A* in the Australian Core Ranking


NLP 顶会
NAACL，ACL

cv顶会
cvpr

通用机器学习顶会
ICLR、ICML，ECCV,NIPS,  ICCV , EMNLP   NLPCC

‘layers’ -> ‘slim’ -> ‘estimators’ -> ‘tf.keras’

ACM International Conference on Web Search and Data Mining (WSDM’


Airbnb 的 Real-time Personalization using Embeddings for Search Ranking at Airbnb 一文拿了今年 KDD ADS Track 的最佳论文，和 16 年 Google 的 W&D 类似，并不 fancy，但非常 practicable


deepfm
            # model
            # -1 256 8    -1 39  =  -1 39*8  
            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K
            # -1  39   =》  -1  39 1
            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])
            #-1*39*8   -1*39*1广播， = -1 *39*8
            self.embeddings = tf.multiply(self.embeddings,feat_value)


me

                #20*40*128
                self.ad_img_embeddings_var  = tf.get_variable("ad_img_embedding_var", [AD_IMG_LABEL_DIM,AD_IMG_VALUE_DIM,EMBEDDING_DIM])
                #索引 idx     -1 *4*（40*128）
                self.ad_img_embedded = tf.nn.embedding_lookup(self.ad_img_embeddings_var, self.ad_label_ph)
                #  -1*4    -1*4*40
                self.ad_value_ph_ohot = tf.one_hot(self.ad_value_ph,depth=AD_IMG_VALUE_DIM,axis=-1)
                # -1*4*40   -1*4*1*40
                self.ad_value_ph_ohot = tf.expand_dims(self.ad_value_ph_ohot,axis=-2)
                #n*7*8*128  就是对应相乘，  -1*4*1*40  -1 *4*（40*128）= -1 *4*1*128
                self.ad_img_embedded = tf.matmul(self.ad_value_ph_ohot ,self.ad_img_embedded)        
                self.ad_img_eb = self.ad_img_embedded     # none*n*1*128     
                self.ad_img_eb = tf.squeeze(self.ad_img_eb,[-2])  #n*n*128

                self.ad_img_eb_sum = tf.reduce_sum(self.ad_img_eb,-2)



multi lable
            for s in Config.multi_features:
                temp_multi_result = tf.nn.embedding_lookup_sparse(self.vr['multi_first_embedding_%s' % s],
                tf.SparseTensor(indices=self.ph['multi_index_%s' % s],
                                values=self.ph['multi_value_%s' % s],
                                dense_shape=(Config.batch_size,
                                            Config.embedding_size)),
                None,
                combiner="sum"
                )

### embedding lookup SparseTensor
idx = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 1], [1, 2], [2, 0]],
                      values=[0, 1, 2, 3, 0], dense_shape=[3, 3])
# 这个稀疏矩阵写成普通形式这样
#---------------------------------------------------------------------#
array([[0, 1, None],     每一行的每列 拿出嵌入向量，相加
       [None, 2, 3],
       [0, None, None]]) # 为了与0元素相区别，没有填充的部分写成了None 



import pymysql

conn = pymysql.connect(host = "218.68.6.114",user = "leapmotion",passwd = 'leapmotion',db = 'leapmotion_data',port =3306)
cur = conn.cursor()

cur.execute('select * from LeapMachine where id <=100')

'''
'(('{json}',id))'
'''
data_all = cur.fetchall()



    # cell = tf.nn.rnn_cell.LSTMCell(HIDDEN_DIM)
    # cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=KEEP_PROB)
    # cells = [cell for _ in range(NUM_LAYERS)]

    def build_cell(n,m):
        cell = tf.nn.rnn_cell.LSTMCell(n)
        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=m)
        return cell

    num_units=[HIDDEN_DIM*2,HIDDEN_DIM]
    
    cells = [build_cell(n,KEEP_PROB) for n in num_units]
    Cell_stacked = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)


字典
    for k, v in aa.items():

    pandas 
    for row in aim.itertuples(index=False):
                        if getattr(row, "label") == 1:



In [102]: data                                                  
Out[102]: {'a': [1, 2, 3], 'c': [4, 5, 6], 'b': [7, 8, 9]}

In [103]: frame = pd.DataFrame(data,index=['one','two','three'])                                                                                             
In [104]: frame                                                                                                
Out[104]: 
       a  c  b
one    1  4  7
two    2  5  8
three  3  6  9

In [110]: for i in range(len(frame)): 
     ...:     tmp = frame.iloc[i]['b'] 
     ...:     print(tmp) 
     ...:                          
7
8
9
这个会报错
# for i in range(len(frame["one"])):
#     print(a["one"][i])



In [162]: tag =2                                                                        
In [163]: for i in range(10): 
     ...:     print(i) 
     ...:     if i ==tag: 
     ...:         break 
     ...: else: 
     ...:     print("xx") 
     ...: print("ok")                                    
0
1
2
ok

In [164]: for i in range(1): 
     ...:     print(i) 
     ...:     if i ==tag: 
     ...:         break 
     ...: else: 
     ...:     print("xx") 
     ...: print("ok")                                                                                             
0
xx
ok


#dien
with tf.name_scope('rnn_1'):
    # self.seq_len_ph 一维，是每个历史数据，的长度，实际长度，不是最大长度，，里面 用作mask的
    rnn_outputs, _ = dynamic_rnn(GRUCell(HIDDEN_SIZE), inputs=self.item_his_eb,
                                    sequence_length=self.seq_len_ph, dtype=tf.float32,
                                    scope="gru1")
with tf.name_scope('Attention_layer_1'):
    #self.mask_ph 二维矩阵，样本 最大长度，有值得为1 其他 为零   
    #注意  这里的mask，是尾部为零，   而历史信息
    att_outputs, alphas = din_fcn_attention(self.item_eb, rnn_outputs, ATTENTION_SIZE, self.mask_ph,
                                            softmax_stag=1, stag='1_1', mode='LIST', return_alphas=True)
with tf.name_scope('rnn_2'):
    rnn_outputs2, final_state2 = dynamic_rnn(VecAttGRUCell(HIDDEN_SIZE), inputs=rnn_outputs,
                                                att_scores=tf.expand_dims(alphas, -1),
                                                sequence_length=self.seq_len_ph, dtype=tf.float32,
                                                scope="gru2")



*/5 * * * * nohup python -u /data/go/src/naga/songofsiren/tools/log/report_api_error.py >> /data/logs/naga/report_by_log.log  2>&1 &
#*/5 * * * * nohup python -u /data/go/src/naga/songofsiren/tools/send_by_online.py >> /data/go/src/naga/songofsiren/tools/send_by_online.log  2>&1 &
0 */1 * * * nohup python -u /data/go/src/naga/songofsiren/tools/report/report_by_susuan_ocrtask.py >> /data/logs/naga/report_by_susaun_ocrtask.log  2>&1 &
0 2 * * * bash /disk/script/online_log_bak.sh >> /disk/script/log/online_log_bak.log 2>&1 &
0 4 * * * nohup python3 /disk/script/parser_log_insert_to_ocrdb.py /disk/disk2 20190101-20191232  >> /disk/script/log/parser_log_insert_to_ocrdb.log 2>&1 &