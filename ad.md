搜索引擎营销 search engine marketing   
品牌曝光量  营销转换量



#RL4Rec
基于强化学习的推荐学习
序列化推荐 强化学习
每条广告的转换率 = 转换次数 / 点击次数

dien 用到了 neg-samplting
#feature representation
- user profile :fileds包括 性别，年龄等
- user behavior：用户浏览的物品id，
- ad： ad_id,shop_id 
- context: 时间

对于Amazon数据 吧用户评论简单看做 是 点击，对哪个商品评论 就是对哪个商品点击，   而不是把评论看做正负 来看做点击

阿里的数据
UV：店铺各页面的访问人数，一个IP在24小时内多次访问店铺被只算一次
PV：24小时内店铺内所有页面的浏览总量，可累加。
IPV：指买家找到您店铺的宝贝后，点击进入宝贝详情页的次数，可累加。
IPV_UV是浏览过商品详情的独立访问者，注意：IPV_UV也是不能累加的。


连续型数据 怎么放入网络呢

#

learning to rank
pagerank，From RankNet to LambdaRank to LambdaMART
个性化特征，
排序学习的目的就是通过一些自动化的方法完成模型参数的训练。根据不同类型的训练数据可以将排序学习方法分为以下三类：a）单点标注（point wise）；b）两两标注（pair wise）；c）列表标注（list wise）


linecache里面最常用到的就是getline方法，简单实用可以直接从内容中读到指定的行，日常编程中如果涉及读取大文件，一定要使用首选linecache模块，相比open()那种方法要快N倍，它是你读取文件的效率之源。



检索，展现，点击，收入，CTR3


目标衡量已完成的操作或转化，转化的示例包括购买或提交联系表单。
所需的转化次数
参与度
A / B测试:目标是确定哪些元素导致访问者更高的参与度和行动，然后隔离此变量。

因为把新的广告产品放在已经在进行广告投放的营销者触手可及的地方永远会是最能把盘子做大的方法

Google Analytics 网站分析工具

营销 渠道 流量

帮助广告主在一个场所购买多个出版商的广告位，并使用位置或时间信息定向广告，可自动将广告插入网页，获取报告


第三方监测工具如doubleclick、秒针等都利用了用户跟踪技术，网站 分析工具如GoogleAnalytics、百度统计、CNZZ等 也利用了用户跟踪 技术。

协议，域名，端口有任何一个的不同，就被当作是跨域

重定向 
反向代理


select poll 轮询

QPS query per second

#TODO
blstm
attenlstm  是对广告，是对点击历史


python高级进阶
 scrapy 
 flask

git进阶
分支 
合并
tag

tensorflow进阶
bert
文本分类， word2vec 分层softmax ngram
tfserving

hash idmb5 

search query tokens


强化学习用于推荐系统，特别是序列化推荐（Sequential Recommendation, SR）是一个直观的解决方案。很多现有的SR文章其实都在做“根据前k个interaction推荐第(k+1)个”的工作[1]，然而要做到真正的序列化推荐，其实更应该采用连续决策（sequential decision making）优化的思路。我认为基于user-item interaction的推荐系统研究接下去有两个大的发展方向，一个是“向后看”，即根据用户过去的长期交互行为，更好地建模用户行为，得到综合的用户画像[2,4]；另一个则是“向前看”，即连续推荐优化综合收益[5，8]。这些综合收益不一定是cumulative user actions，更可以是复合用户行为，例如（点击+转化），或者用户参与度[7]等综合指标。前景一定是广阔的，因为这里面有很多难点没有解决，例如：1）离线评估与在线效果之间的巨大鸿沟，这个鸿沟的背后原因有很多，不是一两个因素所造成。诚如之前答案所述，RL4Rec难以做到客观全面地离线评估，这主要是离线模拟过程的泛化性问题，特别是用户端连续决策行为的建模方面。现有方法其实还是有很多问题，例如没有刻画用户反馈行为的连续性与前后相关性；没有考虑用户反馈背后的多因素影响等等。2）动作空间偏大。在真实的推荐系统中，可能面临巨大无比的离散化行为空间，因为待推荐item集合巨大无比。当然现在的推荐系统会使用粗排再精排，但在我们的讨论上下文中，我倾向于全程使用RL方法。那么这就带来行为空间过大的问题（远大于目前的game environment）。这个问题我们实验室的论文[6]已经作出了一些探索，但还没有完全解决这类问题。3）在线效果容易受到其它策略的影响。例如推荐端在使用RL算法，用户还在看到很多其它推荐、展示策略在其它位置、时段、平台推给他的结果，较难精确建模用户反馈与行为归因。4）如何去做更好地探索，因为真实场景中没有游戏环境那样丰富的样本数据，每一次探索都有巨大的显性成本或隐性成本。更快地探索、更好地采用效率是RL4Rec实际应用中的难题。5）强化学习用于推荐系统，更难的是说服部门领导上线一个真实环境中可能会表现得较为弱智的强化学习策略，因为他需要你证明现在或未来它能产生更大的收益。




模型验证就是在选择模型和超参数之后，通过对训练数据进行学习，对比模型对已知数据的预测值与实际值的差异。模型验证的正确方法是使用留出集评估模型性能，即先从训练模型中的数据中留出一部分，然后用这部分留出来的数据检验模型性能。

但是，使用留出集使得模型失去了一部分训练机会，解决这个问题的方法是交叉验证,也就是做一组拟合，让数据的每个子集既是训练集，又是验证集。

Scikit-Learn为不同的应用场景提供了各种交叉验证方法，都以迭代器形式在corss_validation模块中实现。例如，我们每次只用一个样本做测试，其他样本全用于训练。这种交叉检验类型成为LOO（leave-one-out）交叉验证。

“最优模型”的问题基本上可以看成是找出偏差和方差的平衡点的问题。使用复杂度较低的模型（高偏差）时，训练数据往往欠拟合，说明模型对训练数据和新数据都缺乏预测能力。而使用复杂度较高的模型（高方差）时，训练数据往往过拟合，说明模型对训练数据预测能力强，但是对新数据的预测能力很很差。当使用复杂度适中的模型时，验证曲线得分很高。说明再该模型复杂度条件下，偏差与方差达到均衡状态。





